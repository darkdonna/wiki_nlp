{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fddc9d3",
   "metadata": {},
   "source": [
    "# Reading articles from Wikipedia \n",
    "based on the list of names\n",
    "https://pypi.org/project/wikipedia/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071ddc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31871437",
   "metadata": {},
   "source": [
    "## Scrape articles for Eastern European Female Leaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "37a5c709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sviatlana Tsikhanouskaya</td>\n",
       "      <td>Belarus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Olga Abramova (politician)</td>\n",
       "      <td>Belarus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alena Anisim</td>\n",
       "      <td>Belarus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Natallia Eismant</td>\n",
       "      <td>Belarus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maria Kalesnikava</td>\n",
       "      <td>Belarus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name   country\n",
       "0    Sviatlana Tsikhanouskaya  Belarus \n",
       "1  Olga Abramova (politician)  Belarus \n",
       "2                Alena Anisim  Belarus \n",
       "3            Natallia Eismant  Belarus \n",
       "4           Maria Kalesnikava  Belarus "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the list of Women EE names\n",
    "df = pd.read_csv('EE_name_country.csv', names = ['name', 'country'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8f69c9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Belarus</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bulgaria</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CZ</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hungary</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moldova</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poland</th>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RU</th>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romania</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ukraine</th>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name\n",
       "country       \n",
       "Belarus     15\n",
       "Bulgaria    22\n",
       "CZ          15\n",
       "Hungary     57\n",
       "Moldova     59\n",
       "Poland     142\n",
       "RU         173\n",
       "Romania     54\n",
       "Ukraine    131"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save list of countries\n",
    "country_list = df.country.unique()\n",
    "\n",
    "# Number of articles in initial list by country\n",
    "df.groupby('country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c944c234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239                 Ewa Janik\n",
       "240               Bogna Janke\n",
       "241        Elżbieta Jankowska\n",
       "242       Małgorzata Janowska\n",
       "243    Izabela Jaruga-Nowacka\n",
       "                ...          \n",
       "305         Alicja Olechowska\n",
       "306        Małgorzata Olejnik\n",
       "307           Halina Olendzka\n",
       "308      Daria Gosek-Popiołek\n",
       "309               Dorota Olko\n",
       "Name: name, Length: 71, dtype: object"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = country_list[5]\n",
    "df['name'].iloc[239:][df.country == country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e2930b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page id \"viktoria kinzburska\" does not match any pages. Try another id!\n",
      "Page id \"anna romanov\" does not match any pages. Try another id!\n",
      "Page id \"kirk rudi\" does not match any pages. Try another id!\n",
      "Articles downloaded: 128\n"
     ]
    }
   ],
   "source": [
    "# Save content of Wiki article pages from the list\n",
    "articles = []\n",
    "\n",
    "# Change country \n",
    "country = country_list[-1]\n",
    "\n",
    "for n in df['name'].iloc[239:][df.country == country]:\n",
    "    try:\n",
    "        page = wikipedia.page(n)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(e.options)\n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        print(e)\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        e.status_code = \"Connection refused\"\n",
    "    else:\n",
    "        text = page.content\n",
    "        # Save only articles that are over 800 symbols\n",
    "        if len(text) > 10:\n",
    "            articles.append(text)\n",
    "print('Articles downloaded:', len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ab014028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles downloaded: 128\n",
      "First article: Olha Pavlivna Aivazovska (Ukrainian: Ольга Павлівна Айвазовська; born 9 February 1981 in Zalishchyky\n",
      "Last article: Yaryna Bohdanivna Turchyn (Ukrainian: Турчин Ярина Богданівна, born (1975-10-22)October 22, 1975 in \n"
     ]
    }
   ],
   "source": [
    "# How many articles has been downloaded \n",
    "print('Articles downloaded:', len(articles))\n",
    "# what is the first article\n",
    "print('First article:', articles[0][:100])\n",
    "# what is the last article\n",
    "print('Last article:', articles[-1][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "064b1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save downloaded acticles into CSV file\n",
    "dic = {'articles': articles}\n",
    "articles_df = pd.DataFrame(dic)\n",
    "filename = country +'_articles.csv'\n",
    "articles_df.to_csv('EE_articles/' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ae0e0",
   "metadata": {},
   "source": [
    "## Importing saved articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a54d655e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 655\n",
      "After de-dupe: 653\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Belarus</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bulgaria</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CZ</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hungary</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moldova</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poland</th>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RU</th>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romania</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ukraine</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          articles\n",
       "country           \n",
       "Belarus         14\n",
       "Bulgaria        21\n",
       "CZ              15\n",
       "Hungary         55\n",
       "Moldova         59\n",
       "Poland         141\n",
       "RU             169\n",
       "Romania         51\n",
       "Ukraine        128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the list of Women EE names\n",
    "df = pd.read_csv('EE_name_country.csv', names = ['name', 'country'])\n",
    "#Save list of countries\n",
    "country_list = df.country.unique()\n",
    "\n",
    "# Import all CSV files from all Wiki articles and save them to one list\n",
    "articles_df = pd.DataFrame({'articles': [], 'country': []})\n",
    "\n",
    "for country in country_list:\n",
    "    filename = country +'_*.csv'\n",
    "    files = glob.glob('EE_articles/' + filename)\n",
    "    \n",
    "    all_articles = []\n",
    "    for file in files:\n",
    "        read_handle = open(file, \"r\")\n",
    "        text = list(csv.reader(read_handle, delimiter=\",\"))        \n",
    "        for article in text[1:]:\n",
    "            all_articles.append(article[1])     \n",
    "    \n",
    "    country_df = pd.DataFrame({'articles': all_articles, 'country': country})\n",
    "    articles_df = pd.concat([articles_df, country_df], ignore_index=True)\n",
    "\n",
    "# How many articles has been read\n",
    "print('Number of articles:', len(articles_df))\n",
    "\n",
    "# Drop duplicates\n",
    "articles_df = articles_df.drop_duplicates()\n",
    "\n",
    "# Count the number of rows after\n",
    "lenAfter = len(articles_df)\n",
    "print(\"After de-dupe: \" + str(lenAfter))\n",
    "\n",
    "# Number of articles in initial list by country\n",
    "articles_df.groupby('country').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a26a4",
   "metadata": {},
   "source": [
    "## Cleaning and tokenisation\n",
    "- Removing headers - https://www.w3schools.com/python/python_regex.asp\n",
    "- Removing stop words and numbers\n",
    "- Stemming and lemmitisation\n",
    "- Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b8083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77172c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens groups: 653 \n",
      "\n",
      "-No-   --Tokens--\n",
      "  1      1468\n",
      "  2       976\n",
      "  3       140\n",
      "  4       320\n",
      "  5      1289\n",
      "  6       343\n",
      "  7        31\n",
      "  8       579\n",
      "  9       268\n",
      " 10       197\n",
      " 11       138\n",
      " 12       273\n",
      " 13       280\n",
      " 14       248\n",
      " 15       341\n"
     ]
    }
   ],
   "source": [
    "# Based on https://www.geeksforgeeks.org/nlp-gensim-tutorial-complete-guide-for-beginners/\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "# import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Check for latin alphabet \n",
    "# https://stackoverflow.com/questions/27084617/detect-strings-with-non-english-characters-in-python\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "for article in articles_df['articles']:\n",
    "    # Removing section headers and new line breaks\n",
    "    text = re.sub(\"==.*==\",'', article)\n",
    "    text = re.sub(\"\\n\",'', text)\n",
    "    \n",
    "    # Convert a document into a list of tokens \n",
    "    # This lowercases, tokenizes, removes numerical values\n",
    "    tokens = simple_preprocess(text)\n",
    "    \n",
    "    doc_out = []\n",
    "    for word in tokens:    \n",
    "        if word not in stop_words:  # to remove stopwords\n",
    "            if isEnglish(word):\n",
    "                Lemmatized_Word = wnl.lemmatize(word)  # lemmatize\n",
    "                doc_out.append(Lemmatized_Word)\n",
    "    \n",
    "    all_tokens.append(doc_out)\n",
    "\n",
    "# Print out infromation about articles and number of tokens for top 15\n",
    "print('Tokens groups:', len(all_tokens),'\\n')\n",
    "print(\"{0:7}{1:10}\".format(\"-No-\",\"--Tokens--\"))\n",
    "for x, tokens in enumerate(all_tokens[:15]):\n",
    "    print(\"{0:3}{1:10}\".format(x + 1, len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80f159",
   "metadata": {},
   "source": [
    "# Dictionary - Corpus - LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1689761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models import LdaModel, LdaMulticore, CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6643b3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary length: 1308\n",
      "Top 20 tokens by frequency\n",
      "\n",
      "1. ukraine - 1395\n",
      "2. party - 1343\n",
      "3. russian - 1337\n",
      "4. state - 1142\n",
      "5. ukrainian - 1108\n",
      "6. election - 1088\n",
      "7. deputy - 1003\n",
      "8. minister - 817\n",
      "9. european - 797\n",
      "10. president - 736\n",
      "11. russia - 721\n",
      "12. also - 720\n",
      "13. national - 720\n",
      "14. committee - 718\n",
      "15. elected - 716\n",
      "16. parliament - 700\n",
      "17. parliamentary - 679\n",
      "18. university - 660\n",
      "19. became - 562\n",
      "20. year - 556\n",
      "\n",
      "Corpus created. \n",
      "Corpus length: 653\n"
     ]
    }
   ],
   "source": [
    "# create dictionary - a map of unique tokens\n",
    "dictionary = Dictionary(all_tokens)\n",
    "dictionary.filter_extremes(no_below = 15, no_above = 0.7)\n",
    "print('Dictionary length:', len(dictionary.keys()))\n",
    "\n",
    "# 100 tokens by frequency for cleaned up dictionary\n",
    "t_most_freq = dictionary.most_common(100)\n",
    "print('Top 20 tokens by frequency\\n')\n",
    "\n",
    "num = 1\n",
    "for t, f in t_most_freq[:20]:\n",
    "    print(str(num) + '.', t, '-', f)\n",
    "    num = num + 1\n",
    "\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(token) for token in all_tokens]\n",
    "print('\\nCorpus created. \\nCorpus length:', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e324e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing top 250 tokens for all articles by frequency to a CSV file\n",
    "top_df = pd.DataFrame(dictionary.most_common(250))\n",
    "top_df.columns = ['token', 'frequency']\n",
    "top_df.to_csv('output/EE_frequency_top_250.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "2bb615bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary to disk\n",
    "dictionary.save('EE_Wiki_dictionary.dict')\n",
    "# save corpus to disk\n",
    "MmCorpus.serialize(\"EE_Wiki_corpus.mm\", corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7c5f3",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81802879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ec9577c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term frequency - inverse document frequency model\n",
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7734c72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-10 tokens for article No 87 \n",
      "\n",
      "moldova        0.6224351539558823\n",
      "judge          0.3767640762765586\n",
      "lawyer         0.3346266050167051\n",
      "court          0.24184508916144154\n",
      "jurist         0.1821387024914272\n",
      "republic       0.16931240645894055\n",
      "parliamentary  0.15275787175852834\n",
      "constitutional 0.14905253723405457\n",
      "appointment    0.1461174799751714\n",
      "period         0.1394567227916222\n"
     ]
    }
   ],
   "source": [
    "article = 87\n",
    "print('\\nTop-10 tokens for article No', article, '\\n')\n",
    "tfidf_int = tfidf[corpus[(article - 1)]]\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_int, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 10 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:10]:\n",
    "    print(\"{0:15}{1:10}\".format(dictionary.get(term_id), weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a3d20",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bc5d7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases<129526 vocab, min_count=10, threshold=50, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "# Train a toy phrase model on our training corpus.\n",
    "phrase_model = Phrases(all_tokens, min_count = 10, threshold = 50, connector_words = ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "print(phrase_model)\n",
    "\n",
    "# Finding bigrams in the interview \n",
    "bigrams = phrase_model.find_phrases(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "812a00fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kluzik_rostkowska</td>\n",
       "      <td>3029.847953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>katser_buchkovska</td>\n",
       "      <td>2698.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gronkiewicz_waltz</td>\n",
       "      <td>2502.917874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ivano_frankivsk</td>\n",
       "      <td>2299.278107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>donald_tusk</td>\n",
       "      <td>2055.968254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nizhny_novgorod</td>\n",
       "      <td>1962.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>liliia_hrynevych</td>\n",
       "      <td>1644.774603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sasha_bezuhanova</td>\n",
       "      <td>1555.867868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>record_transcript</td>\n",
       "      <td>1530.223323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dmitry_medvedev</td>\n",
       "      <td>1488.804598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>petro_poroshenko</td>\n",
       "      <td>1484.762594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>real_estate</td>\n",
       "      <td>1446.063796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>khatia_dekanoidze</td>\n",
       "      <td>1428.595588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wayback_machine</td>\n",
       "      <td>1407.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>klaus_iohannis</td>\n",
       "      <td>1349.229167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bigram        score\n",
       "0   kluzik_rostkowska  3029.847953\n",
       "1   katser_buchkovska  2698.458333\n",
       "2   gronkiewicz_waltz  2502.917874\n",
       "3     ivano_frankivsk  2299.278107\n",
       "4         donald_tusk  2055.968254\n",
       "5     nizhny_novgorod  1962.515152\n",
       "6    liliia_hrynevych  1644.774603\n",
       "7    sasha_bezuhanova  1555.867868\n",
       "8   record_transcript  1530.223323\n",
       "9     dmitry_medvedev  1488.804598\n",
       "10   petro_poroshenko  1484.762594\n",
       "11        real_estate  1446.063796\n",
       "12  khatia_dekanoidze  1428.595588\n",
       "13    wayback_machine  1407.891304\n",
       "14     klaus_iohannis  1349.229167"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving Bigrams as a dictionary\n",
    "bigrams_dic = {'bigram': [], 'score': []}\n",
    "for i in bigrams:\n",
    "    bigrams_dic['bigram'].append(i)\n",
    "    bigrams_dic['score'].append(bigrams[i])\n",
    "bigrams_df = pd.DataFrame(bigrams_dic)\n",
    "bigrams_df = bigrams_df.sort_values( by = ['score'], ascending = False)\n",
    "bigrams_df = bigrams_df.reset_index(drop=True)\n",
    "\n",
    "# Write bigrams to a CSV\n",
    "bigrams_df.to_csv('output/ee_full_wiki_bigrams.csv')\n",
    "# Showing top 15 bigrams for all articles\n",
    "bigrams_df.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
